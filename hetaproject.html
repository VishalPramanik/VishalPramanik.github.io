<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>HETA Project | Research Page</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Bootstrap CSS -->
  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
    rel="stylesheet"
  />

  <style>
    :root {
      --bg-light: #e6f3ff;
      --bg-lighter: #f5fbff;
      --card-bg: #ffffff;
      --accent: #0ea5e9;
      --accent-soft: #bae6fd;
      --accent-deep: #0369a1;
      --text-muted: #64748b;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
        sans-serif;
      background-color: var(--bg-light);
    }

    .navbar {
      box-shadow: 0 2px 8px rgba(15, 23, 42, 0.1);
      background: linear-gradient(90deg, #ffffff, #e0f2fe);
    }

    .nav-link {
      font-size: 0.95rem;
      font-weight: 500;
    }

    .nav-link:hover {
      color: var(--accent-deep) !important;
    }

    .hero-section {
      padding: 4.5rem 0 2.5rem;
      background: radial-gradient(circle at top left, #dbeafe, #e0f2fe 50%, #f5fbff 100%);
    }

    .hero-title {
      font-size: clamp(2.4rem, 4vw, 3rem);
      font-weight: 800;
      letter-spacing: -0.03em;
      color: #0f172a;
    }

    .hero-subtitle {
      font-size: 1.05rem;
      color: var(--text-muted);
    }

    .author-list a {
      text-decoration: none;
      color: #0f172a;
    }

    .author-list a:hover {
      text-decoration: underline;
    }

    .badge-equal {
      font-size: 0.7rem;
      background-color: #e0f2fe;
      color: #0f172a;
    }

    .cta-buttons .btn {
      margin-right: 0.5rem;
      margin-bottom: 0.5rem;
    }

    .btn-primary {
      background-color: var(--accent);
      border-color: var(--accent);
    }

    .btn-primary:hover {
      background-color: var(--accent-deep);
      border-color: var(--accent-deep);
    }

    .btn-outline-primary {
      color: var(--accent-deep);
      border-color: var(--accent-soft);
      background-color: rgba(191, 219, 254, 0.25);
    }

    .btn-outline-primary:hover {
      background-color: var(--accent-deep);
      color: #ffffff;
      border-color: var(--accent-deep);
    }

    .btn-outline-success {
      border-color: #86efac;
      color: #15803d;
      background-color: #dcfce7;
    }

    .btn-outline-success:hover {
      background-color: #15803d;
      border-color: #15803d;
      color: #ffffff;
    }

    .teaser-card img {
      border-radius: 0.9rem;
      border: 1px solid var(--accent-soft);
    }

    section {
      padding: 3rem 0;
      background-color: var(--bg-light);
    }

    section:nth-of-type(even) {
      background-color: var(--bg-lighter);
    }

    .section-title {
      font-size: 1.6rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
      color: #0f172a;
    }

    .section-subtitle {
      font-size: 0.95rem;
      color: var(--text-muted);
      margin-bottom: 1.5rem;
    }

    .fig-caption {
      font-size: 0.85rem;
      color: var(--text-muted);
    }

    .resource-card {
      border-radius: 0.9rem;
      border: 1px solid var(--accent-soft);
      background-color: var(--card-bg);
      transition: transform 0.12s ease, box-shadow 0.12s ease;
      height: 100%;
    }

    .resource-card:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 22px rgba(15, 23, 42, 0.12);
    }

    .chip {
      display: inline-flex;
      align-items: center;
      padding: 0.15rem 0.6rem;
      border-radius: 999px;
      font-size: 0.75rem;
      background-color: #e0f2fe;
      color: #0369a1;
      margin-right: 0.3rem;
      margin-bottom: 0.3rem;
    }

    pre.bibtex-block {
      background-color: #0f172a;
      color: #e5e7eb;
      border-radius: 0.6rem;
      padding: 1rem 1.2rem;
      font-size: 0.85rem;
      overflow-x: auto;
      border: 1px solid #1f2937;
    }

    footer {
      padding: 2rem 0 2.5rem;
      font-size: 0.85rem;
      color: var(--text-muted);
      background: linear-gradient(180deg, #e0f2fe, #ffffff);
      border-top: 1px solid #dbeafe;
    }
  </style>
</head>
<body>
  <!-- NAVBAR -->
  <nav class="navbar navbar-expand-lg fixed-top">
    <div class="container">
      <a class="navbar-brand fw-semibold" href="#">HETA Project</a>
      <button
        class="navbar-toggler"
        type="button"
        data-bs-toggle="collapse"
        data-bs-target="#navbarNav"
        aria-controls="navbarNav"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Paper</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#archive">Archive & Code</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#demo">Input / Output Demo</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#overview">Method</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#bibtex">BibTeX</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- HERO / TITLE -->
  <header class="hero-section">
    <div class="container">
      <div class="row align-items-center g-4">
        <div class="col-lg-7">
          <h1 class="hero-title mb-3">
            Hessian-Enhanced Token Attribution (HETA)
          </h1>
          <p class="hero-subtitle mb-3">
            <span class="author-list">
              <a href="#">Vishal Pramanik*</a>,
              <a href="#">Maisha Maliha*</a>,
              <a href="#">Sumit Kumar Jha</a>
            </span>
            <span class="badge badge-equal ms-2">* equal contribution</span>
          </p>
          <p class="hero-subtitle mb-1">
            <strong>Florida International University</strong>
          </p>
          <p class="hero-subtitle mb-3">
            Preprint • Under review
          </p>

          <div class="cta-buttons mb-3">
            <!-- Update href targets to your real links -->
            <a href="assets/heta_paper.pdf" class="btn btn-primary" target="_blank">
              Paper (PDF)
            </a>
            <a href="#" class="btn btn-outline-primary" target="_blank">
              arXiv / Archive
            </a>
            <a href="#" class="btn btn-outline-secondary disabled">
              Code – coming soon
            </a>
          </div>
        </div>

        <div class="col-lg-5">
          <!-- Teaser Figure -->
          <div class="card teaser-card shadow-sm">
            <!-- Replace src with your teaser image path -->
            <img
              src="assets/images/heta_teaser_placeholder.png"
              class="card-img-top"
              alt="HETA overview diagram placeholder"
            />
            <div class="card-body">
              <p class="fig-caption mb-0">
                High-level view of HETA: semantic flow traces causal paths, curvature measures
                second-order sensitivity, and KL divergence captures information loss under
                token masking. Replace this image with your actual figure.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- ABSTRACT -->
  <section id="abstract">
    <div class="container">
      <h2 class="section-title">Abstract</h2>
      <p class="section-subtitle">
        Concise summary of the HETA framework and its contributions.
      </p>

      <p>
        Modern decoder-only language models generate fluent text but often behave as opaque
        “black boxes”. Many popular attribution methods were originally developed for
        encoder-style architectures and rely on first-order or linear approximations, which
        struggle to explain autoregressive generation. HETA addresses this gap by providing
        token-level attributions that are explicitly tailored to decoder-only models.
      </p>
      <p>
        The framework combines three complementary signals. First, a semantic transition
        component traces attention–value flows that end at the target position, enforcing a
        causal gate over tokens that can influence the chosen output. Second, a Hessian-based
        term captures second-order curvature of the target log-likelihood with respect to token
        embeddings, revealing nonlinear and interaction effects that gradients alone may miss.
        Third, a KL divergence term measures how the predictive distribution over the target
        changes when individual tokens are masked, providing an information-theoretic view of
        importance. Together, these pieces yield context-aware, causally grounded, and
        semantically meaningful explanations for generative language models. Empirical
        evaluations across several models and datasets show that HETA improves attribution
        faithfulness and agreement with human annotations over prior methods.
      </p>
    </div>
  </section>

  <!-- ARCHIVE / CODE (NO DATASET CARD) -->
  <section id="archive">
    <div class="container">
      <h2 class="section-title">Paper Archive & Code</h2>
      <p class="section-subtitle">
        Links to the current version of the paper and the upcoming open-source release.
      </p>

      <div class="row g-4">
        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">Paper PDF</h5>
            <p class="mb-3">
              Link to the main manuscript describing the HETA framework, methodology,
              and experimental results.
            </p>
            <a href="assets/heta_paper.pdf" class="btn btn-sm btn-primary w-100">
              Download Paper
            </a>
          </div>
        </div>

        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">arXiv / Archive</h5>
            <p class="mb-3">
              Once available, this will point to the public preprint or conference archive
              version of the paper.
            </p>
            <a href="#" class="btn btn-sm btn-outline-primary w-100">
              Archive link – coming soon
            </a>
          </div>
        </div>

        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">Code</h5>
            <p class="mb-3">
              The official implementation of HETA, including evaluation scripts and
              reproducing the reported experiments, will be released here.
            </p>
            <a href="#" class="btn btn-sm btn-outline-secondary w-100 disabled">
              Code – coming soon
            </a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- INPUT / OUTPUT DEMO -->
  <section id="demo">
    <div class="container">
      <h2 class="section-title">Interactive Demo: Input / Output</h2>
      <p class="section-subtitle">
        This section is designed for plugging in a live Gradio or HuggingFace Space demo.
        Replace the placeholder URL in the iframe with your hosted demo link.
      </p>

      <div class="row g-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Try HETA Online</h5>
              <p class="card-text">
                The interactive interface lets you type a prompt, select a target token, and
                visualize token-level attributions. Tokens are colour-coded so that darker hues
                correspond to higher influence on the chosen prediction.
              </p>
              <ul class="mb-3">
                <li>Enter an input sequence for a decoder-only language model.</li>
                <li>Choose a target token or index for attribution.</li>
                <li>Inspect how HETA distributes credit across the context.</li>
              </ul>
              <p class="mb-2">
                <strong>Where to put your link:</strong> in the iframe on the right, replace
                <code>YOUR_DEMO_URL_HERE</code> with your Gradio or HuggingFace Space URL.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Embedded Demo</h5>
              <p class="card-text">
                The live demo can be embedded directly below. Users can interact with it
                without leaving the project page.
              </p>
              <div class="ratio ratio-16x9 border rounded" style="background-color:#e0f2fe;">
                <!-- TODO: Replace YOUR_DEMO_URL_HERE with your Gradio or HuggingFace Space URL -->
                <iframe
                  src="https://YOUR_DEMO_URL_HERE"
                  title="HETA Gradio/HuggingFace Demo"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen
                  style="border:0;"
                ></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MOTIVATION -->
  <section id="motivation">
    <div class="container">
      <h2 class="section-title">Motivation</h2>
      <p class="section-subtitle">
        Why existing attribution methods struggle with decoder-only language models, and
        what HETA aims to fix.
      </p>

      <p>
        Interpreting which tokens matter for a generative model’s prediction is challenging.
        Attention maps are visually appealing, but they track where the model “looks”, not
        necessarily what truly affects the final logit. They can assign high weight to tokens
        whose removal barely changes the output, and they often blur together direct and
        indirect influence paths through deep stacks of layers and residual connections.
      </p>
      <p>
        Classical attribution techniques based on first-order derivatives face a different
        limitation. Gradients and integrated gradients assume that a local linear
        approximation around the input is informative. In highly nonlinear regions, or
        where gradients vanish despite the function being sensitive to finite changes,
        these methods under-report important tokens. This is especially problematic in
        autoregressive models, where each new token is generated conditioned on a long and
        context-dependent history.
      </p>
      <p>
        HETA is motivated by these gaps. The framework is designed to respect causal masking
        in decoder-only transformers, to account for curvature in the log-likelihood surface,
        and to measure how much predictive information a token contributes when it is
        removed. The goal is not just a heatmap, but a set of explanations that remain
        stable, faithful, and aligned with human judgments across prompts, architectures,
        and decoding settings.
      </p>
    </div>
  </section>

  <!-- METHOD OVERVIEW -->
  <section id="overview">
    <div class="container">
      <h2 class="section-title">Method Overview</h2>
      <p class="section-subtitle">
        HETA decomposes token influence into semantic flow, curvature-based sensitivity,
        and information-theoretic impact.
      </p>

      <div class="row g-4 mb-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your illustration -->
            <img
              src="assets/images/heta_semantic_flow.png"
              class="card-img-top"
              alt="Semantic flow diagram placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Semantic Transition Influence</h5>
              <p class="card-text">
                HETA first traces attention–value flows that terminate at the current target
                position under the decoder’s causal mask. This yields a semantic transition
                vector that assigns non-negative mass only to tokens that lie on valid paths
                to the target. The vector serves as a gate: attributions can only be assigned
                to tokens that can, in principle, influence the prediction through the model’s
                architecture.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your illustration -->
            <img
              src="assets/images/heta_hessian_kl.png"
              class="card-img-top"
              alt="Hessian and KL impact diagram placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Curvature & Information Gain</h5>
              <p class="card-text">
                To capture nonlinear effects, HETA estimates token-wise sensitivity using
                Hessian–vector products and Hutchinson-style estimators, avoiding explicit
                construction of the full Hessian. In parallel, it measures how the target
                distribution changes when each token is masked, via KL divergence between
                the original and perturbed predictions. The final attribution score multiplies
                the causal gate with a weighted sum of curvature and information terms,
                producing a target-conditioned importance score for each token.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="row g-4">
        <div class="col-lg-12">
          <div class="card h-100">
            <!-- Replace with your illustration -->
            <img
              src="assets/images/heta_pipeline_full.png"
              class="card-img-top"
              alt="Full HETA pipeline placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Putting It All Together</h5>
              <p class="card-text">
                The unified pipeline produces non-negative, target-specific attribution scores
                for every token in the context. The design balances three perspectives:
                structural (which tokens are causally connected), geometric (how the
                log-likelihood curve bends around the input), and information-theoretic
                (how much probability mass shifts when a token is removed). This combination
                leads to explanations that are both empirically robust and theoretically
                grounded for decoder-only language models.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS SECTION -->
  <section id="results">
    <div class="container">
      <h2 class="section-title">Results</h2>
      <p class="section-subtitle">
        Summary of experimental findings across models, benchmarks, and curated evaluation
        datasets.
      </p>

      <div class="row g-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your results figure -->
            <img
              src="assets/images/heta_benchmarks.png"
              class="card-img-top"
              alt="Benchmark results placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Attribution Faithfulness</h5>
              <p class="card-text">
                HETA is evaluated on benchmark datasets such as LongRA, TellMeWhy, and
                WikiBio using multiple decoder-only language models (including mid-sized and
                large transformers). On perturbation-based metrics like Soft-NC and Soft-NS,
                HETA consistently achieves higher scores than gradient-based, attention-only,
                and recent attribution baselines. The gains are especially pronounced on
                tasks that require long-range reasoning and multi-sentence context.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your qualitative figure -->
            <img
              src="assets/images/heta_curated_dataset.png"
              class="card-img-top"
              alt="Curated dataset and robustness placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Alignment & Robustness</h5>
              <p class="card-text">
                A curated evaluation set combines narrative passages with question–answer
                pairs to test whether attribution mass concentrates on truly diagnostic
                evidence rather than distractors. Using a Dependent Sentence Attribution
                metric, HETA assigns substantially more importance to answer-bearing spans
                than competing methods. Additional analyses show that HETA’s attributions
                remain stable under input noise, active/passive rephrasings, and changes to
                decoding hyperparameters, whereas many baselines fluctuate significantly.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BIBTEX -->
  <section id="bibtex">
    <div class="container">
      <h2 class="section-title">BibTeX</h2>
      <p class="section-subtitle">
        Citation information will be posted here once the paper is publicly available.
      </p>

      <pre class="bibtex-block">BibTeX entry – coming soon.</pre>
    </div>
  </section>

  <!-- FOOTER -->
  <footer>
    <div class="container">
      <div class="row gy-2 align-items-center">
        <div class="col-md-8">
          <span>
            © <span id="year"></span> HETA Project Authors. All rights
            reserved. Developed by <strong>Vishal Pramanik</strong>.
          </span>
        </div>
        <div class="col-md-4 text-md-end">
          <span>
            Template inspired by open-source academic project pages.
          </span>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    // set footer year
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
