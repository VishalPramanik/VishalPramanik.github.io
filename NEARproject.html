<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Fact or Hallucination? | Shapley NEAR</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Bootstrap CSS -->
  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
    rel="stylesheet"
  />

  <style>
    :root {
      --bg-light: #e6f3ff;
      --bg-lighter: #f5fbff;
      --card-bg: #ffffff;
      --accent: #0ea5e9;
      --accent-soft: #bae6fd;
      --accent-deep: #0369a1;
      --text-muted: #64748b;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
        sans-serif;
      background-color: var(--bg-light);
    }

    .navbar {
      box-shadow: 0 2px 8px rgba(15, 23, 42, 0.1);
      background: linear-gradient(90deg, #ffffff, #e0f2fe);
    }

    .nav-link {
      font-size: 0.95rem;
      font-weight: 500;
    }

    .nav-link:hover {
      color: var(--accent-deep) !important;
    }

    .hero-section {
      padding: 4.5rem 0 2.5rem;
      background: radial-gradient(circle at top left, #dbeafe, #e0f2fe 50%, #f5fbff 100%);
    }

    .hero-title {
      font-size: clamp(2.2rem, 3.8vw, 3rem);
      font-weight: 800;
      letter-spacing: -0.03em;
      color: #0f172a;
    }

    .hero-subtitle {
      font-size: 1.05rem;
      color: var(--text-muted);
    }

    .author-list a {
      text-decoration: none;
      color: #0f172a;
    }

    .author-list a:hover {
      text-decoration: underline;
    }

    .cta-buttons .btn {
      margin-right: 0.5rem;
      margin-bottom: 0.5rem;
    }

    .btn-primary {
      background-color: var(--accent);
      border-color: var(--accent);
    }

    .btn-primary:hover {
      background-color: var(--accent-deep);
      border-color: var(--accent-deep);
    }

    .btn-outline-primary {
      color: var(--accent-deep);
      border-color: var(--accent-soft);
      background-color: rgba(191, 219, 254, 0.25);
    }

    .btn-outline-primary:hover {
      background-color: var(--accent-deep);
      color: #ffffff;
      border-color: #0369a1;
    }

    .btn-outline-secondary {
      border-color: #cbd5f5;
      color: #6b7280;
      background-color: #e5e7eb;
    }

    .btn-outline-secondary:hover {
      background-color: #6b7280;
      color: #ffffff;
      border-color: #6b7280;
    }

    /* Figure in Abstract section */
    .heta-figure-container {
      width: 100%;
      max-width: 1200px;
      margin: 1.5rem auto 2rem;
      text-align: center;
    }

    .heta-figure {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
      border: none;
      box-shadow: none;
      background: transparent;
    }

    section {
      padding: 3rem 0;
      background-color: var(--bg-light);
    }

    section:nth-of-type(even) {
      background-color: var(--bg-lighter);
    }

    .section-title {
      font-size: 1.6rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
      color: #0f172a;
    }

    .section-subtitle {
      font-size: 0.95rem;
      color: var(--text-muted);
      margin-bottom: 1.5rem;
    }

    .fig-caption {
      font-size: 0.85rem;
      color: var(--text-muted);
    }

    .resource-card {
      border-radius: 0.9rem;
      border: 1px solid var(--accent-soft);
      background-color: var(--card-bg);
      transition: transform 0.12s ease, box-shadow 0.12s ease;
      height: 100%;
    }

    .resource-card:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 22px rgba(15, 23, 42, 0.12);
    }

    pre.bibtex-block {
      background-color: #0f172a;
      color: #e5e7eb;
      border-radius: 0.6rem;
      padding: 1rem 1.2rem;
      font-size: 0.85rem;
      overflow-x: auto;
      border: 1px solid #1f2937;
    }

    footer {
      padding: 2rem 0 2.5rem;
      font-size: 0.85rem;
      color: var(--text-muted);
      background: linear-gradient(180deg, #e0f2fe, #ffffff);
      border-top: 1px solid #dbeafe;
    }
  </style>
</head>
<body>
  <!-- NAVBAR -->
  <nav class="navbar navbar-expand-lg fixed-top">
    <div class="container">
      <a class="navbar-brand fw-semibold" href="#">Shapley NEAR</a>
      <button
        class="navbar-toggler"
        type="button"
        data-bs-toggle="collapse"
        data-bs-target="#navbarNav"
        aria-controls="navbarNav"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Paper</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#archive">Archive & Code</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#demo">Demo</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#overview">Method</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#bibtex">BibTeX</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- HERO / TITLE -->
  <header class="hero-section">
    <div class="container">
      <div class="row g-4">
        <div class="col-12">
          <h1 class="hero-title mb-3 text-center">
            Fact or Hallucination? An Entropy-Based Framework for Attention-Wise Usable Information in LLMs
          </h1>
          <p class="hero-subtitle mb-2 text-center">
            <strong>Shapley NEAR</strong> – Norm-basEd Attention-wise usable infoRmation
          </p>
          <p class="hero-subtitle mb-2 text-center">
            <span class="author-list">
              <!-- Replace with real author names when de-anonymized -->
              <a href="#">Anonymous Submission</a>
            </span>
          </p>
          <p class="hero-subtitle mb-1 text-center">
            <!-- Replace with actual affiliation if desired -->
            <strong>Affiliation to be updated</strong>
          </p>

          <div class="cta-buttons mb-3 d-flex justify-content-center flex-wrap">
            <!-- Update href targets to your real links -->
            <a href="near_paper.pdf" class="btn btn-primary" target="_blank">
              Paper (PDF)
            </a>
            <!-- Replace YOUR_ARCHIVE_LINK_HERE with your actual arXiv / conference link -->
            <a href="YOUR_ARCHIVE_LINK_HERE" class="btn btn-outline-primary" target="_blank">
              arXiv / Archive
            </a>
            <a href="#" class="btn btn-outline-secondary disabled">
              Code – coming soon
            </a>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- ABSTRACT (WITH OVERVIEW FIGURE) -->
  <section id="abstract">
    <div class="container">
      <h2 class="section-title">Abstract</h2>

      <div class="heta-figure-container">
        <!-- Overview figure of NEAR; replace src with your own overview image if desired -->
        <img
          src="NEAR_fig.jpg"
          alt="Overview of the Shapley NEAR framework for hallucination detection"
          class="heta-figure img-fluid"
        />
        <p class="fig-caption mt-2 text-center">
          Overview of Shapley NEAR: norm-based attention outputs are converted to entropy-based information gain
          across layers and heads, which is then fairly attributed to context sentences via Shapley values to
          assess hallucination risk.
        </p>
      </div>

      <p>
        Large language models can confidently generate incorrect answers, which is risky in safety-critical
        applications. Many existing hallucination detectors only use final-layer logits or post-hoc textual checks,
        ignoring the semantic structure encoded in intermediate attention blocks. Shapley NEAR addresses this by
        defining an entropy-based attribution framework over all layers and heads, grounded in V-usable information
        and Shapley value theory.
      </p>
      <p>
        The method converts norm-based attention outputs into head- and layer-wise information gain, comparing
        entropy with and without context. This information is then decomposed into sentence-level contributions
        using Shapley values, producing a NEAR score that serves as a confidence signal for the model’s answer:
        high NEAR scores indicate that context genuinely reduced uncertainty, while low scores flag likely
        hallucinations.
      </p>
      <p>
        Shapley NEAR further distinguishes between <em>parametric hallucinations</em> (the model’s pre-trained
        knowledge overriding context) and <em>context-induced hallucinations</em> (misleading context spuriously
        boosting confidence), and supports a test-time head clipping strategy to disable attention heads that
        consistently behave in a hallucination-prone, context-agnostic way.
      </p>
    </div>
  </section>

  <!-- ARCHIVE / CODE -->
  <section id="archive">
    <div class="container">
      <h2 class="section-title">Paper Archive & Code</h2>
      <p class="section-subtitle">
        Links to the latest version of the paper, public preprint, and the planned codebase.
      </p>

      <div class="row g-4">
        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">Paper PDF</h5>
            <p class="mb-3">
              Complete manuscript describing Shapley NEAR: definitions, theoretical properties, experimental
              setup, and extensive ablations.
            </p>
            <a href="near_paper.pdf" class="btn btn-sm btn-primary w-100" target="_blank">
              Download Paper
            </a>
          </div>
        </div>

        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">arXiv / Archive</h5>
            <p class="mb-3">
              Once public, this link can point to the arXiv preprint or conference proceedings version of the work.
            </p>
            <a href="YOUR_ARCHIVE_LINK_HERE" class="btn btn-sm btn-outline-primary w-100" target="_blank">
              View Archive
            </a>
          </div>
        </div>

        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">Code</h5>
            <p class="mb-3">
              Planned open-source release implementing NEAR, sentence-level Shapley attribution, and head clipping
              on top of popular LLMs.
            </p>
            <a href="#" class="btn btn-sm btn-outline-secondary w-100 disabled">
              Code – coming soon
            </a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- DEMO (UPDATED WITH YOUR GRADIO LINK) -->
  <section id="demo">
    <div class="container">
      <h2 class="section-title">Interactive Demo (Optional)</h2>
      <p class="section-subtitle">
        This section can host a live NEAR demo (e.g., via Gradio or Streamlit) showing how NEAR flags
        hallucinated answers for question–context pairs.
      </p>

      <div class="row g-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Demo Description</h5>
              <p class="card-text">
                A typical demo would let users input a context passage and question, query a selected LLM
                (such as Qwen2.5-3B, LLaMA3.1-8B, or OPT-6.7B), and then visualize the NEAR score alongside
                sentence-level attributions. Sentences with high contribution to information gain are highlighted.
              </p>
              <ul class="mb-3">
                <li>Paste or select a context and question.</li>
                <li>Generate the model’s answer and compute NEAR.</li>
                <li>View sentence-level Shapley NEAR scores and a global NEAR confidence score.</li>
              </ul>
              <p class="mb-0">
                Try the live demo
                <a href="https://4f96de1aab78c66c20.gradio.live/" target="_blank" rel="noopener noreferrer">
                  here
                </a>.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Embedded Demo</h5>
              <p class="card-text">
                You can embed a live NEAR demo directly below.
              </p>
              <div class="ratio ratio-16x9 border rounded" style="background-color:#e0f2fe;">
                <iframe
                  src="https://4f96de1aab78c66c20.gradio.live/"
                  title="Shapley NEAR Demo"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen
                  style="border:0;"
                ></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MOTIVATION -->
  <section id="motivation">
    <div class="container">
      <h2 class="section-title">Motivation</h2>
      <p class="section-subtitle">
        Why entropy-based, attention-wise usable information is needed for hallucination detection.
      </p>

      <p>
        LLM hallucinations emerge when models output fluent but incorrect statements with high confidence,
        especially problematic in domains where factual accuracy is crucial. Token-level entropy signals and
        semantic diversity of multiple generations can be useful, but extending them cleanly to sentence-level
        decisions in autoregressive models is non-trivial, and they typically overlook what happens inside the
        network.
      </p>
      <p>
        Prior work on V-usable information and pointwise V-information showed that classical mutual information
        tends to overestimate how much signal a computationally bounded model can actually exploit. At the same
        time, analyses of transformer internals revealed that feed-forward layers often encode superficial
        correlations, while attention heads are more aligned with in-context reasoning.
      </p>
      <p>
        Shapley NEAR brings these threads together: it focuses on attention outputs, measures how much they reduce
        entropy relative to a null context, and then attributes this usable information to individual context
        sentences. This gives an interpretable, plug-and-play signal that correlates with whether an answer is
        supported by the context or likely to be hallucinated.
      </p>
    </div>
  </section>

  <!-- METHOD OVERVIEW -->
  <section id="overview">
    <div class="container">
      <h2 class="section-title">Method Overview</h2>
      <p class="section-subtitle">
        From norm-based attention information to sentence-level Shapley NEAR scores and head clipping.
      </p>

      <div class="row g-4 mb-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Norm-Based Attention Information & Information Gain</h5>
              <p class="card-text">
                For each layer ℓ and head h, the model computes query, key, and value tensors over the concatenated
                context and question. The attention output for head (ℓ, h) is projected to the model dimension, and
                the vector at the final question token is extracted. Its norm is used as a proxy for information
                carried by that head, and a softmax over this vector defines a vocabulary distribution.
              </p>
              <p class="card-text">
                Entropy of this distribution is measured with and without the context (null input), and the
                difference defines an <em>information gain</em> for that head. Summing over all layers and heads yields
                a total information gain IG(x → q) capturing how much the context reduces predictive uncertainty
                for the question.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Sentence-Level Shapley Attribution & NEAR</h5>
              <p class="card-text">
                The context is segmented into sentences, and the total information gain is viewed as a cooperative
                “game” between these segments. Using Shapley values, NEAR computes the average marginal contribution
                of each sentence across all permutations of the remaining sentences.
              </p>
              <p class="card-text">
                The <strong>Shapley NEAR score</strong> is the average of these sentence-level Shapley values. It is bounded
                by a function of the number of layers, heads, and vocabulary size, symmetric across sentences with
                identical contributions, and empirically monotone when more layers are included. An AME-style
                estimator with randomly sampled coalitions provides a practical approximation with proven error
                bounds.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="row g-4">
        <div class="col-lg-12">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Parametric vs Context-Induced Hallucinations & Head Clipping</h5>
              <p class="card-text">
                NEAR also helps distinguish different failure modes. When a context sentence that does not contain
                the answer reduces information gain (negative Shapley value), it signals <em>parametric hallucination</em>:
                the model’s internal knowledge conflicts with the context and increases uncertainty. When such a
                sentence spuriously raises information gain (positive contribution), it indicates
                <em>context-induced hallucination</em> where misleading context overly boosts confidence.
              </p>
              <p class="card-text">
                By tracking head-wise contributions, NEAR identifies attention heads that consistently show
                strongly negative information gain. Clipping these heads at test time improves hallucination
                detection and answer quality (e.g., higher AUROC, accuracy, and ROUGE-L on CoQA with LLaMA3.1-8B),
                demonstrating how an interpretability-based analysis can directly inform low-cost, training-free
                interventions.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS SECTION -->
  <section id="results">
    <div class="container">
      <h2 class="section-title">Results</h2>
      <p class="section-subtitle">
        Performance on QA benchmarks, Shapley ablations, threshold analysis, and head clipping, along with
        concrete examples of NEAR on answerable and unanswerable questions.
      </p>

      <!-- TWO EXAMPLE IMAGES: POSITIVE (ANSWERABLE) & NEGATIVE (UNANSWERABLE), SIDE BY SIDE -->
      <div class="row g-4 mb-4">
        <div class="col-md-6 text-center">
          <img
            src="near_answerable.jpg"
            alt="Positive NEAR example where the question is answerable from the context"
            class="img-fluid rounded shadow-sm"
            style="max-width: 100%; height: auto;"
          />
          <p class="fig-caption mt-2">
            Positive NEAR example: the question is answerable from the context. The NEAR visualization shows
            high scores on the supporting sentences, indicating strong, context-grounded usable information.
          </p>
        </div>
        <div class="col-md-6 text-center">
          <img
            src="near_unanswerable.jpg"
            alt="Negative NEAR example where the question is not answerable from the context"
            class="img-fluid rounded shadow-sm"
            style="max-width: 100%; height: auto;"
          />
          <p class="fig-caption mt-2">
            Negative NEAR example: the question is not answerable from the context. NEAR assigns low scores to
            all sentences, signalling a high risk of hallucination despite the model's fluent response.
          </p>
        </div>
      </div>

      <!-- TEXT RESULTS (UNCHANGED) -->
      <div class="row g-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Hallucination Detection Performance</h5>
              <p class="card-text">
                NEAR is evaluated on CoQA, QuAC, SQuAD v2.0 (unanswerable subset), and TriviaQA (rc-nocontext)
                using Qwen2.5-3B, LLaMA3.1-8B, and OPT-6.7B. Metrics include AUROC, Kendall’s τ, and Pearson
                correlation between NEAR scores and ground-truth answerability labels.
              </p>
              <p class="card-text">
                Across all models and datasets, NEAR consistently outperforms strong baselines, often surpassing
                INSIDE by about 8–13% in AUROC and by 10–15% in rank and linear correlation metrics. The best
                scores are obtained on SQuAD, suggesting that when the underlying QA task is easier, NEAR’s
                attention-wise signal translates into particularly clean separation between confident, correct
                answers and hallucinations.
              </p>
              <p class="card-text">
                Additional experiments on larger models (e.g., LLaMA-3.1-70B and Phi-3-Medium-14B) and long-context
                benchmarks such as LongRA further support NEAR’s robustness and scalability (shown in the appendix).
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Shapley Aggregation, Thresholds, and Head Clipping</h5>
              <p class="card-text">
                Replacing the Shapley aggregation with a simple greedy ranking over sentence-level information gain
                reduces performance. With Shapley, AUROC, Kendall’s τ, and PCC on CoQA + LLaMA3.1-8B improve
                substantially (e.g., AUROC from ≈0.79 to ≈0.85 and τ from ≈0.51 to ≈0.66), showing that coalition-aware
                attribution is important for stable rankings.
              </p>
              <p class="card-text">
                Sweeping NEAR thresholds across quantiles reveals that the first quartile (Q1) gives the most reliable
                separation between answerable and hallucinated cases across all datasets and models, while thresholds
                near 0 or far into the upper tail hurt accuracy.
              </p>
              <p class="card-text">
                Finally, clipping heads whose information gain falls below a strong negative threshold leads to
                additional gains. On CoQA with LLaMA3.1-8B, NEAR+Head Clipping improves AUROC and accuracy over both
                NEAR alone and INSIDE, and yields better ROUGE-L alignment between generated and reference answers,
                highlighting the practical usefulness of NEAR-driven structural interventions.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BIBTEX -->
  <section id="bibtex">
    <div class="container">
      <h2 class="section-title">BibTeX</h2>
      <p class="section-subtitle">
        Add the final BibTeX entry here once the paper is public and de-anonymized.
      </p>

      <pre class="bibtex-block">
@inproceedings{NEAR2025,
  title     = {Fact or Hallucination? An Entropy-Based Framework
               for Attention-Wise Usable Information in LLMs},
  author    = {To be updated},
  booktitle = {To be updated},
  year      = {2025}
}
      </pre>
    </div>
  </section>

  <!-- FOOTER -->
  <footer>
    <div class="container">
      <div class="row gy-2 align-items-center">
        <div class="col-md-8">
          <span>
            © <span id="year"></span> Shapley NEAR Authors. All rights
            reserved. Webpage developed by <strong>Your Name</strong>.
          </span>
        </div>
        <div class="col-md-4 text-md-end">
          <span>
            Template adapted from open-source academic project pages.
          </span>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    // set footer year
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
