<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>HETA Project | Research Page</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Bootstrap CSS -->
  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
    rel="stylesheet"
  />

  <style>
    :root {
      --bg-light: #e6f3ff;
      --bg-lighter: #f5fbff;
      --card-bg: #ffffff;
      --accent: #0ea5e9;
      --accent-soft: #bae6fd;
      --accent-deep: #0369a1;
      --text-muted: #64748b;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
        sans-serif;
      background-color: var(--bg-light);
    }

    .navbar {
      box-shadow: 0 2px 8px rgba(15, 23, 42, 0.1);
      background: linear-gradient(90deg, #ffffff, #e0f2fe);
    }

    .nav-link {
      font-size: 0.95rem;
      font-weight: 500;
    }

    .nav-link:hover {
      color: var(--accent-deep) !important;
    }

    .hero-section {
      padding: 4.5rem 0 2.5rem;
      background: radial-gradient(circle at top left, #dbeafe, #e0f2fe 50%, #f5fbff 100%);
    }

    .hero-title {
      font-size: clamp(2.4rem, 4vw, 3rem);
      font-weight: 800;
      letter-spacing: -0.03em;
      color: #0f172a;
    }

    .hero-subtitle {
      font-size: 1.05rem;
      color: var(--text-muted);
    }

    .author-list a {
      text-decoration: none;
      color: #0f172a;
    }

    .author-list a:hover {
      text-decoration: underline;
    }

    .badge-equal {
      font-size: 0.7rem;
      background-color: #e0f2fe;
      color: #0f172a;
    }

    .cta-buttons .btn {
      margin-right: 0.5rem;
      margin-bottom: 0.5rem;
    }

    .btn-primary {
      background-color: var(--accent);
      border-color: var(--accent);
    }

    .btn-primary:hover {
      background-color: var(--accent-deep);
      border-color: var(--accent-deep);
    }

    .btn-outline-primary {
      color: var(--accent-deep);
      border-color: var(--accent-soft);
      background-color: rgba(191, 219, 254, 0.25);
    }

    .btn-outline-primary:hover {
      background-color: var(--accent-deep);
      color: #ffffff;
      border-color: var(--accent-deep);
    }

    .btn-outline-secondary {
      border-color: #cbd5f5;
      color: #6b7280;
      background-color: #e5e7eb;
    }

    .btn-outline-secondary:hover {
      background-color: #6b7280;
      color: #ffffff;
      border-color: #6b7280;
    }

    /* Figure in Abstract section: full width, responsive across devices */
    .heta-figure-container {
      width: 100%;
      max-width: 1200px;
      margin: 1.5rem auto 2rem;
      text-align: center;
    }

    .heta-figure {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
      border: none;
      box-shadow: none;
      background: transparent;
    }

    section {
      padding: 3rem 0;
      background-color: var(--bg-light);
    }

    section:nth-of-type(even) {
      background-color: var(--bg-lighter);
    }

    .section-title {
      font-size: 1.6rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
      color: #0f172a;
    }

    .section-subtitle {
      font-size: 0.95rem;
      color: var(--text-muted);
      margin-bottom: 1.5rem;
    }

    .fig-caption {
      font-size: 0.85rem;
      color: var(--text-muted);
    }

    .resource-card {
      border-radius: 0.9rem;
      border: 1px solid var(--accent-soft);
      background-color: var(--card-bg);
      transition: transform 0.12s ease, box-shadow 0.12s ease;
      height: 100%;
    }

    .resource-card:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 22px rgba(15, 23, 42, 0.12);
    }

    pre.bibtex-block {
      background-color: #0f172a;
      color: #e5e7eb;
      border-radius: 0.6rem;
      padding: 1rem 1.2rem;
      font-size: 0.85rem;
      overflow-x: auto;
      border: 1px solid #1f2937;
    }

    footer {
      padding: 2rem 0 2.5rem;
      font-size: 0.85rem;
      color: var(--text-muted);
      background: linear-gradient(180deg, #e0f2fe, #ffffff);
      border-top: 1px solid #dbeafe;
    }
  </style>
</head>
<body>
  <!-- NAVBAR -->
  <nav class="navbar navbar-expand-lg fixed-top">
    <div class="container">
      <a class="navbar-brand fw-semibold" href="#">HETA Project</a>
      <button
        class="navbar-toggler"
        type="button"
        data-bs-toggle="collapse"
        data-bs-target="#navbarNav"
        aria-controls="navbarNav"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Paper</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#archive">Archive & Code</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#demo">Input / Output Demo</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#overview">Method</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#bibtex">BibTeX</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- HERO / TITLE (NO IMAGE HERE) -->
  <header class="hero-section">
    <div class="container">
      <div class="row g-4">
        <div class="col-12">
          <h1 class="hero-title mb-3 text-center">
            Hessian-Enhanced Token Attribution (HETA)
          </h1>
          <p class="hero-subtitle mb-2 text-center">
            <span class="author-list">
              <a href="#">Vishal Pramanik*</a>,
              <a href="#">Maisha Maliha*</a>,
              <a href="#">Sumit Kumar Jha</a>
            </span>
            <span class="badge badge-equal ms-2">* equal contribution</span>
          </p>
          <p class="hero-subtitle mb-1 text-center">
            <strong>Florida International University</strong>
          </p>
          <p class="hero-subtitle mb-3 text-center">
            NeurIPS 2025 Workshop on Foundations of Reasoning in Language Models (FoRLM) &mdash; Under review
          </p>

          <div class="cta-buttons mb-3 d-flex justify-content-center flex-wrap">
            <!-- Update href targets to your real links -->
            <a href="assets/heta_paper.pdf" class="btn btn-primary" target="_blank">
              Paper (PDF)
            </a>
            <a href="#" class="btn btn-outline-primary" target="_blank">
              arXiv / Archive
            </a>
            <a href="#" class="btn btn-outline-secondary disabled">
              Code – coming soon
            </a>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- ABSTRACT (WITH FULL-WIDTH JPG FIGURE) -->
  <section id="abstract">
    <div class="container">
      <h2 class="section-title">Abstract</h2>
      
      <!-- MAIN HETA FIGURE: JPG in SAME folder as this HTML -->
      <div class="heta-figure-container">
        <!--
          IMPORTANT:
          Put HETA_fig_webpage.jpg in the SAME folder as this HTML file.
          If your filename is different, change it in the src below.
        -->
        <img
          src="HETA_fig_webpage.jpg"
          alt="Overview of the HETA framework"
          class="heta-figure img-fluid"
        />
        <p class="fig-caption mt-2 text-center">
          Overview of HETA: semantic influence (target-conditioned attention–value rollout),
          curvature sensitivity (Hessian-based estimates), and information gain (KL divergence
          under token masking) combine to produce target-specific token attributions.
        </p>
      </div>

      <!-- ABSTRACT TEXT -->
      <p>
        Decoder-only language models generate impressive text, but their decisions are
        difficult to interpret. Most existing attribution methods were designed for
        encoder-style architectures and rely on local, first-order approximations, which
        struggle to capture the causal and semantic structure of autoregressive generation.
        As a result, their explanations can be unstable and misaligned with true token
        influence.
      </p>
      <p>
        HETA addresses this limitation with a token-level attribution framework tailored
        to decoder-only language models. It combines three complementary views of
        importance: (i) a <em>semantic transition</em> component that traces attention–value
        flows ending at the target token, enforcing a causal gate over tokens that can
        influence the prediction; (ii) a <em>Hessian-based sensitivity</em> term that captures
        second-order curvature of the target log-likelihood, revealing nonlinear and
        interaction effects beyond gradients; and (iii) an <em>information-theoretic</em> term
        that measures how the predictive distribution changes when individual tokens are
        masked. Together, these signals yield context-aware, causally grounded, and
        semantically meaningful attributions that outperform strong baselines on both
        benchmark datasets and curated evaluation setups.
      </p>
    </div>
  </section>

  <!-- ARCHIVE / CODE -->
  <section id="archive">
    <div class="container">
      <h2 class="section-title">Paper Archive & Code</h2>
      <p class="section-subtitle">
        Links to the current version of the paper and the planned open-source release.
      </p>

      <div class="row g-4">
        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">Paper PDF</h5>
            <p class="mb-3">
              Main manuscript describing the HETA framework, theoretical foundations, and
              experimental results.
            </p>
            <a href="HETA_paper.pdf" class="btn btn-sm btn-primary w-100">
              Download Paper
            </a>
          </div>
        </div>

        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">arXiv / Archive</h5>
            <p class="mb-3">
              This link will point to the official preprint or conference archive once it
              is public.
            </p>
            <a href="#" class="btn btn-sm btn-outline-primary w-100">
              Archive link – coming soon
            </a>
          </div>
        </div>

        <div class="col-md-6 col-lg-4">
          <div class="resource-card p-3 h-100">
            <h5 class="mb-2">Code</h5>
            <p class="mb-3">
              The official implementation of HETA, including evaluation pipelines and
              reproducing the reported experiments, will be released after camera-ready.
            </p>
            <a href="#" class="btn btn-sm btn-outline-secondary w-100 disabled">
              Code – coming soon
            </a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- INPUT / OUTPUT DEMO -->
  <section id="demo">
    <div class="container">
      <h2 class="section-title">Interactive Demo: Input / Output</h2>
      <p class="section-subtitle">
        Use this section to embed a live Gradio or HuggingFace Space demo of HETA.
      </p>

      <div class="row g-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Try HETA Online</h5>
              <p class="card-text">
                The demo allows users to provide an input sequence, choose a target token,
                and visualize token-level attributions. Tokens are coloured by their
                HETA score, with darker green indicating higher influence.
              </p>
              <ul class="mb-3">
                <li>Type or paste a prompt for a decoder-only language model.</li>
                <li>Select the index or position of the target token.</li>
                <li>Inspect how HETA distributes attribution across the context.</li>
              </ul>
              <p class="mb-2">
                <strong>Where to put your link:</strong> the iframe on the right already uses your Gradio URL.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <div class="card-body">
              <h5 class="card-title">Embedded Demo</h5>
              <p class="card-text">
                The live demo can be embedded directly below so visitors can interact with
                HETA without leaving this page.
              </p>
              <div class="ratio ratio-16x9 border rounded" style="background-color:#e0f2fe;">
                <!-- Gradio demo URL -->
                <iframe
                  src="https://bbddc68e11073f7781.gradio.live/"
                  title="HETA Gradio Demo"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen
                  style="border:0;"
                ></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MOTIVATION -->
  <section id="motivation">
    <div class="container">
      <h2 class="section-title">Motivation</h2>
      <p class="section-subtitle">
        Why attention maps and first-order gradients are not enough for decoder-only language models.
      </p>

      <p>
        Attention-based explanations show where a model “looks” but not necessarily what
        truly drives its predictions. Attention weights can be rearranged or perturbed
        without significantly changing the output, and aggregated attention across heads
        and layers often mixes direct and indirect influence in ways that are hard to
        interpret.
      </p>
      <p>
        First-order methods such as plain gradients, Input×Gradient, or Integrated
        Gradients approximate influence by local linear sensitivity. In highly nonlinear
        regions or flat regimes of the activation function, gradients can vanish even when
        finite perturbations to a token still cause meaningful changes in the output
        distribution. In autoregressive models, where each token is generated conditioned
        on a long and context-dependent history, these issues become more severe.
      </p>
      <p>
        HETA is motivated by the need for attributions that respect causal structure,
        capture higher-order effects, and reflect how the output distribution actually
        changes when context tokens are perturbed. The framework is designed to provide
        stable, faithful, and interpretable explanations across prompts, models, and
        decoding hyperparameters.
      </p>
    </div>
  </section>

  <!-- METHOD OVERVIEW -->
  <section id="overview">
    <div class="container">
      <h2 class="section-title">Method Overview</h2>
      <p class="section-subtitle">
        HETA decomposes token influence into semantic flow, curvature-based sensitivity,
        and information-theoretic impact.
      </p>

      <div class="row g-4 mb-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your real image if you have one -->
            <img
              src="assets/images/heta_semantic_flow.png"
              class="card-img-top"
              alt="Semantic flow diagram placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Semantic Transition Influence</h5>
              <p class="card-text">
                HETA first traces attention–value flows that terminate at the target
                position under the decoder’s causal mask. This produces a semantic
                transition vector that assigns non-negative mass only to tokens that lie
                on valid paths to the target. It acts as a causal gate: only tokens that
                can structurally influence the target are eligible to receive attribution.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your real image if you have one -->
            <img
              src="assets/images/heta_hessian_kl.png"
              class="card-img-top"
              alt="Hessian and KL impact diagram placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Curvature & Information Gain</h5>
              <p class="card-text">
                To capture nonlinear effects, HETA estimates token-wise sensitivity from
                Hessian–vector products using a Hutchinson estimator, avoiding explicit
                construction of the full Hessian. In parallel, it measures how the target
                distribution changes when each token is masked, via KL divergence between
                the original and perturbed predictions. The final score multiplies the
                causal gate with a weighted combination of curvature and information terms,
                yielding a target-conditioned importance measure for each token.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="row g-4">
        <div class="col-lg-12">
          <div class="card h-100">
            <!-- Replace with your real image if you have one -->
            <img
              src="assets/images/heta_pipeline_full.png"
              class="card-img-top"
              alt="Full HETA pipeline placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Putting It All Together</h5>
              <p class="card-text">
                The overall pipeline yields non-negative, target-specific attribution scores
                for every token in the context. By combining structural (causal paths),
                geometric (curvature), and information-theoretic (KL divergence) views,
                HETA provides explanations that are both theoretically grounded and
                empirically robust for decoder-only language models.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS SECTION -->
  <section id="results">
    <div class="container">
      <h2 class="section-title">Results</h2>
      <p class="section-subtitle">
        Summary of experimental findings across models, benchmarks, and curated evaluation datasets.
      </p>

      <div class="row g-4">
        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your actual benchmark figure -->
            <img
              src="assets/images/heta_benchmarks.png"
              class="card-img-top"
              alt="Benchmark results placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Attribution Faithfulness</h5>
              <p class="card-text">
                On benchmark datasets such as LongRA, TellMeWhy, and WikiBio, HETA
                achieves higher Soft-NC and Soft-NS scores than gradient-based, attention
                rollout, and recent attribution baselines. The improvements are especially
                strong for long-range reasoning tasks, where capturing deep context
                dependencies is crucial.
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-6">
          <div class="card h-100">
            <!-- Replace with your curated dataset / robustness figure -->
            <img
              src="assets/images/heta_curated_dataset.png"
              class="card-img-top"
              alt="Curated dataset and robustness placeholder"
            />
            <div class="card-body">
              <h5 class="card-title">Alignment & Robustness</h5>
              <p class="card-text">
                A curated evaluation set combining narrative and science QA passages is
                used to probe whether attribution mass concentrates on truly diagnostic
                evidence. Using the Dependent Sentence Attribution (DSA) metric, HETA
                substantially outperforms all baselines, while also showing higher
                stability under input noise, syntactic rephrasings, and changes in decoding
                hyperparameters.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BIBTEX -->
  <section id="bibtex">
    <div class="container">
      <h2 class="section-title">BibTeX</h2>
      <p class="section-subtitle">
        Citation information will be posted here once the paper is publicly available.
      </p>

      <pre class="bibtex-block">BibTeX entry – coming soon.</pre>
    </div>
  </section>

  <!-- FOOTER -->
  <footer>
    <div class="container">
      <div class="row gy-2 align-items-center">
        <div class="col-md-8">
          <span>
            © <span id="year"></span> HETA Project Authors. All rights
            reserved. Developed by <strong>Vishal Pramanik</strong>.
          </span>
        </div>
        <div class="col-md-4 text-md-end">
          <span>
            Template inspired by open-source academic project pages.
          </span>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    // set footer year
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
